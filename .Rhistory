######################
#all samples
asv_tab.keeps = asv_tab[,!colnames(asv_tab) %in% toss_these]
ncol(asv_tab.keeps) == ncol(asv_tab) - length(toss_these)
ncol(asv_tab.keeps)
nrow(asv_tab.keeps)
# the ncol is 4 greater than expected from subtracting the lengths bc there are
# four PCR negatives that do not appear in the ASV tab bc they had no reads at all
# there are also four samples missing based on the 444 above. These are samps
# that had zero reads
metadata %>% filter(reads_per_sample < 1000)
metadata
metadata %>% filter(num_reads < 1000)
metadata.NY = read.csv("data/2024_metadata/collated/metadata.NY.csv")
metadata.OH = read.csv("data/2024_metadata/collated/metadata.OH.csv")
metadata.IA = read.csv("data/2024_metadata/collated/metadata.IA.csv")
metadata.NH = read.csv("data/2024_metadata/collated/metadata.NH.csv")
colnames(metadata.NH)
colnames(metadata.IA)
metadata = rbind(metadata.NY, metadata.OH, metadata.IA, metadata.NH)
ncol(metadata)
id_mapping = read.csv("data/2024_metadata/collated/sampleID_mapping_NY-OH-IA-NH_02102025.csv")
reads_per_sample = read.table("data/2024_metadata/collated/reads_per_sample_NY-OH-IA-NH_02102025.txt", header = F)
colnames(reads_per_sample) = c("SequenceID", "num_reads")
head(reads_per_sample)
head(id_mapping)
metadata.NH$trapID
id_mapping[id_mapping$SequenceID %>% grep("^NH", .),]
full_metadata = full_join(id_mapping, metadata, by = "trapID") %>%
full_join(., reads_per_sample, by = "SequenceID")
head(full_metadata)
full_join(id_mapping, metadata, by = "trapID")
reads_per_sample
full_metadata = full_join(id_mapping, metadata, by = "trapID") %>%
full_join(., reads_per_sample, by = "SequenceID")
head(full_metadata)
nrow(full_metadata)
nrow(reads_per_sample)
reads_per_sample = read.table("data/2024_metadata/collated/reads_per_sample_NY-OH-IA-NH_02102025.txt", header = F)
nrow(reads_per_sample)
library(dplyr)
metadata.NY = read.csv("data/2024_metadata/collated/metadata.NY.csv")
metadata.OH = read.csv("data/2024_metadata/collated/metadata.OH.csv")
metadata.IA = read.csv("data/2024_metadata/collated/metadata.IA.csv")
metadata.NH = read.csv("data/2024_metadata/collated/metadata.NH.csv")
colnames(metadata.NH)
colnames(metadata.IA)
metadata = rbind(metadata.NY, metadata.OH, metadata.IA, metadata.NH)
ncol(metadata)
id_mapping = read.csv("data/2024_metadata/collated/sampleID_mapping_NY-OH-IA-NH_02102025.csv")
reads_per_sample = read.table("data/2024_metadata/collated/reads_per_sample_NY-OH-IA-NH_02102025.txt", header = F)
colnames(reads_per_sample) = c("SequenceID", "num_reads")
head(reads_per_sample)
head(id_mapping)
metadata.NH$trapID
id_mapping[id_mapping$SequenceID %>% grep("^NH", .),]
nrow(reads_per_sample)
full_metadata = full_join(id_mapping, metadata, by = "trapID") %>%
full_join(., reads_per_sample, by = "SequenceID")
head(full_metadata)
nrow(full_metadata)
sampleID.split = strsplit(full_metadata$sampleID, " ", fixed = T)
full_metadata$dateOrBaselineType = lapply(sampleID.split, function(x) x[2]) %>% unlist
full_metadata$SequenceID = sub("-", ".", full_metadata$SequenceID)
write.csv(full_metadata %>% select(-BaselineType), "data/2024_metadata/collated/collated_metadata_NY-OH-IA-NH_02102025.csv", row.names = F)
q(save="no")
library(dplyr)
source("library/library.R")
#asv_tab = read.table("~/FEDRR_all_2024/dada2_core/ASVs_counts.tsv", header = T)
asv_tab = read.table("data/FEDRR_all_2024/ASVs_counts.tsv", header = T)
ncol(asv_tab)
nrow(asv_tab)
colnames(asv_tab)
rownames(asv_tab)
#mod names to get rid of Illumina index ID
colnames(asv_tab) = sub("_S\\d+", "", colnames(asv_tab),perl = T)
#funnel trap metadata
metadata = read.csv("data/2024_metadata/collated/collated_metadata_NY-OH-IA-NH_02102025.csv")
head(metadata)
nrow(metadata)
metadata$dateOrBaselineType %>% unique()
library(dplyr)
metadata.NY = read.csv("data/2024_metadata/collated/metadata.NY.csv")
metadata.OH = read.csv("data/2024_metadata/collated/metadata.OH.csv")
metadata.IA = read.csv("data/2024_metadata/collated/metadata.IA.csv")
metadata.NH = read.csv("data/2024_metadata/collated/metadata.NH.csv")
colnames(metadata.NH)
colnames(metadata.IA)
metadata = rbind(metadata.NY, metadata.OH, metadata.IA, metadata.NH)
ncol(metadata)
id_mapping = read.csv("data/2024_metadata/collated/sampleID_mapping_NY-OH-IA-NH_02102025.csv")
reads_per_sample = read.table("data/2024_metadata/collated/reads_per_sample_NY-OH-IA-NH_02102025.txt", header = F)
colnames(reads_per_sample) = c("SequenceID", "num_reads")
head(reads_per_sample)
head(id_mapping)
metadata.NH$trapID
id_mapping[id_mapping$SequenceID %>% grep("^NH", .),]
nrow(reads_per_sample)
full_metadata = full_join(id_mapping, metadata, by = "trapID") %>%
full_join(., reads_per_sample, by = "SequenceID")
head(full_metadata)
nrow(full_metadata)
sampleID.split = strsplit(full_metadata$sampleID, " ", fixed = T)
full_metadata$dateOrBaselineType = lapply(sampleID.split, function(x) x[2]) %>% unlist
full_metadata$SequenceID = sub("-", ".", full_metadata$SequenceID)
write.csv(full_metadata %>% select(-BaselineType), "data/2024_metadata/collated/collated_metadata_NY-OH-IA-NH_02102025.csv", row.names = F)
library(dplyr)
source("library/library.R")
#asv_tab = read.table("~/FEDRR_all_2024/dada2_core/ASVs_counts.tsv", header = T)
asv_tab = read.table("data/FEDRR_all_2024/ASVs_counts.tsv", header = T)
ncol(asv_tab)
nrow(asv_tab)
colnames(asv_tab)
rownames(asv_tab)
#mod names to get rid of Illumina index ID
colnames(asv_tab) = sub("_S\\d+", "", colnames(asv_tab),perl = T)
#funnel trap metadata
metadata = read.csv("data/2024_metadata/collated/collated_metadata_NY-OH-IA-NH_02102025.csv")
head(metadata)
nrow(metadata)
metadata$dateOrBaselineType %>% unique()
baseline_ids = metadata %>% filter(dateOrBaselineType == "BLO" | dateOrBaselineType == "BLF") %>% pull(SequenceID)
baseline_ids %>% length
pcrNeg_ids = metadata %>% filter(dateOrBaselineType == "negative") %>% pull(SequenceID)
pcrNeg_ids %>% length
samples_ids = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType != "PCRnegative" & !SequenceID %in% c("NY.42", "NY.35")) %>% pull(SequenceID)
samples_ids %>% length
samples_ids
#note the samps does not include Eli's samples
#we need to negative filter based on BL and PCR neg
# also add Ny.42 and NY.35
toss_these = c(baseline_ids, pcrNeg_ids, "NY.42", "NY.35")
length(toss_these)
ncol(asv_tab) - length(toss_these)
nrow(metadata) - length(toss_these) + length( grep("^X", colnames(asv_tab) ))
#444
######################
#all samples
asv_tab.keeps = asv_tab[,!colnames(asv_tab) %in% toss_these]
ncol(asv_tab.keeps) == ncol(asv_tab) - length(toss_these)
ncol(asv_tab.keeps)
nrow(asv_tab.keeps)
metadata %>% filter(num_reads < 1000)
# the ncol is 4 greater than expected from subtracting the lengths bc there are
# four PCR negatives that do not appear in the ASV tab bc they had no reads at all
# there are also four samples missing based on the 444 above. These are samps
# that had zero reads
metadata %>% filter(num_reads == 0 & dateOrBaselineType != c("BLO", "BLF", "negative"))
# the ncol is 4 greater than expected from subtracting the lengths bc there are
# four PCR negatives that do not appear in the ASV tab bc they had no reads at all
# there are also four samples missing based on the 444 above. These are samps
# that had zero reads
metadata %>% filter(num_reads == 0 & dateOrBaselineType %in% c("BLO", "BLF", "negative"))
# the ncol is 4 greater than expected from subtracting the lengths bc there are
# four PCR negatives that do not appear in the ASV tab bc they had no reads at all
# there are also four samples missing based on the 444 above. These are samps
# that had zero reads
metadata %>% filter(num_reads == 0 & !dateOrBaselineType %in% c("BLO", "BLF", "negative"))
#how many samples with gt 1k seqs
sum(colSums(asv_tab.keeps) > 999)
#437
colSums(asv_tab.keeps) %>% sort
#
#how many nonzero asvs
sum(rowSums(asv_tab.keeps) > 0)
#Checking just the ST samples
asv_tab.ST = asv_tab[,grep("^X", colnames(asv_tab)) ]
ncol(asv_tab.ST)
#34
sum(colSums(asv_tab.ST) > 999)
#34
colSums(asv_tab.ST) %>% sort
#
#how many nonzero asvs
sum(rowSums(asv_tab.ST) > 0)
#13355
sum(rowSums(asv_tab.ST) > 0)/sum(rowSums(asv_tab.keeps) > 0)
#0.406483
asv_tab.nST = asv_tab[,-grep("^X", colnames(asv_tab)) ]
ncol(asv_tab.nST)
#504
sum(colSums(asv_tab.nST) > 999)
#451
colSums(asv_tab.nST) %>% sort
#
#how many nonzero asvs
sum(rowSums(asv_tab.nST) > 0)
#32135
sum(rowSums(asv_tab.nST) > 0)/sum(rowSums(asv_tab.keeps) > 0)
#
#remove 0 count asvs and samples with lt 1k seqs and transpose. Note we will also output the un filtered/unrarefied samples for purposes of *detection*
#note need to filter samples before ASVs otherwise may retain some more zero count ASVs
asv_tab.keeps = asv_tab.keeps[, colSums(asv_tab.keeps) > 999]
asv_tab.keeps = asv_tab.keeps[rowSums(asv_tab.keeps) > 0, ] %>% t()
nrow(asv_tab.keeps)
ncol(asv_tab.keeps)
max(rowSums(asv_tab.keeps))/min(rowSums(asv_tab.keeps))
###############
#perform rarefactions
#
write.csv(asv_tab.keeps, "data/FEDRR_all_2024/asv_tab.funnel_traps_and_spore_traps.csv", quote = F)
q(save="no")
library(dplyr)
source("library/library.R")
asv_tab = read.table("data/FEDRR_all_2024/ASVs_counts.tsv", header = T)
ncol(asv_tab)
nrow(asv_tab)
colnames(asv_tab)
rownames(asv_tab)
#mod names to get rid of Illumina index ID
colnames(asv_tab) = sub("_S\\d+", "", colnames(asv_tab),perl = T)
#right away get rid of Eli spore trap samples. She is doing that analysis
colnames(asv_tab)
#right away get rid of Eli spore trap samples. She is doing that analysis
grep("^X", colnames(asv_tab))
#right away get rid of Eli spore trap samples. She is doing that analysis
colnames(asv_tab)[grep("^X", colnames(asv_tab))]
#right away get rid of Eli spore trap samples. She is doing that analysis
ST_samps = colnames(asv_tab)[grep("^X", colnames(asv_tab))]
ncol(asv_tab)
#538
nrow(asv_tab)
#right away get rid of Eli spore trap samples. She is doing that analysis
ST_samps = colnames(asv_tab)[grep("^X", colnames(asv_tab))]
length(ST_samps)
#34 ST samps
asv_tab = asv_tab[,colnames(asv_tab) %in% ST_samps]
ncol(asv_tab)
asv_tab = read.table("data/FEDRR_all_2024/ASVs_counts.tsv", header = T)
ncol(asv_tab)
#538
nrow(asv_tab)
# 33678
colnames(asv_tab)
rownames(asv_tab)
#mod names to get rid of Illumina index ID
colnames(asv_tab) = sub("_S\\d+", "", colnames(asv_tab),perl = T)
#right away get rid of Eli spore trap samples. She is doing that analysis
ST_samps = colnames(asv_tab)[grep("^X", colnames(asv_tab))]
#34 ST samps
asv_tab = asv_tab[,!colnames(asv_tab) %in% ST_samps]
ncol(asv_tab)
ST_samps
asv_tab = read.table("data/FEDRR_all_2024/ASVs_counts.tsv", header = T)
ncol(asv_tab)
#538
nrow(asv_tab)
# 33678
colnames(asv_tab)
#rownames(asv_tab)
#right away get rid of Eli spore trap samples. She is doing that analysis
ST_samps = colnames(asv_tab)[grep("^X", colnames(asv_tab))]
length(ST_samps)
#34 ST samps
asv_tab = asv_tab[,!colnames(asv_tab) %in% ST_samps]
ncol(asv_tab)
#mod names to get rid of Illumina index ID
colnames(asv_tab) = sub("_S\\d+", "", colnames(asv_tab),perl = T)
#metadata
metadata = read.csv("data/2024_metadata/collated/collated_metadata_NY-OH-IA-NH_02102025.csv")
head(metadata)
nrow(metadata)
#sample type IDs
baseline_ids = metadata %>% filter(dateOrBaselineType == "BLO" | dateOrBaselineType == "BLF") %>% pull(SequenceID)
baseline_ids %>% length
pcrNeg_ids = metadata %>% filter(dateOrBaselineType == "negative") %>% pull(SequenceID)
pcrNeg_ids %>% length
samples_ids = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType != "PCRnegative" & !SequenceID %in% c("NY.42", "NY.35")) %>% pull(SequenceID) # NY.35 and 42 are a duplicate (sample run twice) and an untraceable sample ID, respectively
samples_ids %>% length
samps_and_baseline_ids = metadata %>% filter(dateOrBaselineType != "negative") %>% pull(SequenceID)
samps_and_baseline_ids %>% length
samps_and_baseline_ids.NH = metadata %>% filter(dateOrBaselineType != "negative" & State == "NH") %>% pull(SequenceID)
samps_and_baseline_ids.NH %>% length
samps_and_baseline_ids %>% length
baseline_ids.NH = metadata %>% filter(dateOrBaselineType == "BLO" | dateOrBaselineType == "BLF" & State == "NH") %>% pull(SequenceID)
baseline_ids.NH %>% length
baseline_ids.NH
baseline_ids.NH = metadata %>% filter((dateOrBaselineType == "BLO" | dateOrBaselineType == "BLF") & State == "NH") %>% pull(SequenceID)
baseline_ids.NH %>% length
#30
baseline_ids.NH
#sample type IDs
samples_ids = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType == "BLF" & dateOrBaselineType != "negative" & !SequenceID %in% c("NY.42", "NY.35")) %>% pull(SequenceID) # NY.35 and 42 are a duplicate (sample run twice) and an untraceable sample ID, respectively
samples_ids %>% length
#sample type IDs
samples_ids = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType == "BLF" & dateOrBaselineType != "negative" & !SequenceID %in% c("NY.42", "NY.35")) %>% pull(SequenceID) # NY.35 and 42 are a duplicate (sample run twice) and an untraceable sample ID, respectively
samples_ids %>% length
metadata
metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType == "BLF" & dateOrBaselineType != "negative" & !SequenceID %in% c("NY.42", "NY.35"))
#sample type IDs
samples_ids = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType != "BLF" & dateOrBaselineType != "negative" & !SequenceID %in% c("NY.42", "NY.35")) %>% pull(SequenceID) # NY.35 and 42 are a duplicate (sample run twice) and an untraceable sample ID, respectively
samples_ids %>% length
samps_and_baseline_ids = metadata %>% filter(dateOrBaselineType != "negative") %>% pull(SequenceID)
samps_and_baseline_ids %>% length
samps_and_pcrNeg_ids = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType == "BLF") %>% pull(SequenceID)
samps_and_pcrNeg_ids %>% length
samps_and_pcrNeg_ids = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType != "BLF") %>% pull(SequenceID)
samps_and_pcrNeg_ids %>% length
samps_and_baseline_ids.NH = metadata %>% filter(dateOrBaselineType != "negative" & State == "NH") %>% pull(SequenceID)
samps_and_baseline_ids.NH %>% length
baseline_ids.NH = metadata %>% filter((dateOrBaselineType == "BLO" | dateOrBaselineType == "BLF") & State == "NH") %>% pull(SequenceID)
baseline_ids.NH %>% length
#30
baseline_ids.NH
######################
######################
#trap catch samples (i.e., the real deal)
asv_tab.samps = asv_tab[,colnames(asv_tab) %in% samples_ids]
ncol(asv_tab.samps) == length(samples_ids)
ncol(asv_tab.samps)
#406
samples_ids[!samples_ids  %in% colnames(asv_tab.samps)]
#how many samples with lt 1k seqs
asv_tab.samps[,colSums(asv_tab.samps) > 999] %>% colnames
asv_tab.samps[,colSums(asv_tab.samps) > 999] %>% ncol
#403 (of 406 that had good seqs)
colSums(asv_tab.samps) %>% sort
#
#how many nonzero asvs
sum(rowSums(asv_tab.samps) > 0)
#31236
sum(rowSums(asv_tab.samps) > 1)
#30726
#
#remove 0 count asvs and samples with lt 1k seqs and transpose. Note we will also output the un filtered/unrarefied samples for purposes of *detection*
asv_tab.samps = asv_tab.samps[, colSums(asv_tab.samps) > 999]
asv_tab.samps = asv_tab.samps[rowSums(asv_tab.samps) > 0,] %>% t()
ncol(asv_tab.samps)
nrow(asv_tab.samps)
######################
#trap catch AND baseline samples
asv_tab.samps_and_baseline = asv_tab[,colnames(asv_tab) %in% samps_and_baseline_ids]
ncol(asv_tab.samps_and_baseline) == length(samps_and_baseline_ids)
samps_and_baseline_ids[!samps_and_baseline_ids  %in% colnames(asv_tab.samps_and_baseline)]
#how many samples with lt 1k seqs
asv_tab.samps_and_baseline[,colSums(asv_tab.samps_and_baseline) > 999] %>% colnames %>% length
#445
colSums(asv_tab.samps_and_baseline) %>% sort
#
#how many nonzero asvs
sum(rowSums(asv_tab.samps_and_baseline) > 0)
#32127
sum(rowSums(asv_tab.samps_and_baseline) > 1)
#31741
#
#remove 0 count asvs and samples with lt 1k seqs and transpose. Note we will also output the un filtered/unrarefied samples for purposes of *detection*
asv_tab.samps_and_baseline = asv_tab.samps_and_baseline[, colSums(asv_tab.samps_and_baseline) > 999]
asv_tab.samps_and_baseline = asv_tab.samps_and_baseline[rowSums(asv_tab.samps_and_baseline) > 0,] %>% t()
ncol(asv_tab.samps_and_baseline)
nrow(asv_tab.samps_and_baseline)
max(rowSums(asv_tab.baseline))/min(rowSums(asv_tab.baseline))
######################
#trap catch AND PCR negatives
asv_tab.samps_and_pcrNeg = asv_tab[,colnames(asv_tab) %in% samps_and_pcrNeg_ids]
ncol(asv_tab.samps_and_pcrNeg) == length(samps_and_pcrNeg_ids)
samps_and_pcrNeg_ids[!samps_and_pcrNeg_ids  %in% colnames(asv_tab.samps_and_pcrNeg)]
#how many samples with lt 1k seqs
asv_tab.samps_and_pcrNeg[,colSums(asv_tab.samps_and_pcrNeg) > 999] %>% colnames %>% length
#409
colSums(asv_tab.samps_and_pcrNeg) %>% sort
#
#how many nonzero asvs
sum(rowSums(asv_tab.samps_and_pcrNeg) > 0)
#31239
sum(rowSums(asv_tab.samps_and_pcrNeg) > 1)
#30730
#
#remove 0 count asvs and samples with lt 1k seqs and transpose. Note we will also output the un filtered/unrarefied samples for purposes of *detection*
asv_tab.samps_and_pcrNeg = asv_tab.samps_and_pcrNeg[, colSums(asv_tab.samps_and_pcrNeg) > 999]
asv_tab.samps_and_pcrNeg = asv_tab.samps_and_pcrNeg[rowSums(asv_tab.samps_and_pcrNeg) > 0,] %>% t()
ncol(asv_tab.samps_and_pcrNeg)
nrow(asv_tab.samps_and_pcrNeg)
######################
#trap catch AND baseline samples IN NH
asv_tab.samps_and_baseline.NH = asv_tab[,colnames(asv_tab) %in% samps_and_baseline_ids.NH]
ncol(asv_tab.samps_and_baseline.NH) == length(samps_and_baseline_ids.NH)
samps_and_baseline_ids.NH[!samps_and_baseline_ids.NH  %in% colnames(asv_tab.samps_and_baseline.NH)]
#how many samples with lt 1k seqs
asv_tab.samps_and_baseline.NH[,colSums(asv_tab.samps_and_baseline.NH) > 999] %>% colnames %>% length
#114
colSums(asv_tab.samps_and_baseline.NH) %>% sort
#how many samples with lt 1k seqs
asv_tab.baseline.NH[,colSums(asv_tab.baseline.NH) > 999] %>% colnames %>% length
######################
#trap catch AND baseline samples IN NH
asv_tab.baseline.NH = asv_tab[,colnames(asv_tab) %in% baseline_ids.NH ]
ncol(asv_tab.baseline.NH) == length(baseline_ids.NH )
baseline_ids.NH [!baseline_ids.NH   %in% colnames(asv_tab.baseline.NH)]
#how many samples with lt 1k seqs
asv_tab.baseline.NH[,colSums(asv_tab.baseline.NH) > 999] %>% colnames %>% length
#14
# these are mostly baselines samples, and
colSums(asv_tab.baseline.NH) %>% sort
# thesamples that don't pass the 1k filter are mostly baselines samples, and
# we would like to compare these. Let's reduce the filter to min 500
asv_tab.baseline.NH[,colSums(asv_tab.baseline.NH) > 499] %>% colnames %>% length
#14
colSums(asv_tab.baseline.NH) %>% sort
ncol(asv_tab.samps_and_baseline.NH)
nrow(asv_tab.samps_and_baseline.NH)
ncol(asv_tab.baseline.NH)
# the samples that don't pass the 1k filter account for more than half of total,
# and we would like to compare these. Let's reduce the filter to min 500
# We will do the same above for the NH samps plus bl
asv_tab.baseline.NH[,colSums(asv_tab.baseline.NH) > 499] %>% colnames %>% length
#
#how many nonzero asvs
sum(rowSums(asv_tab.baseline.NH) > 0)
#32127
sum(rowSums(asv_tab.baseline.NH) > 1)
#1419
#
#remove 0 count asvs and samples with lt 1k seqs and transpose. Note we will also output the un filtered/unrarefied samples for purposes of *detection*
asv_tab.baseline.NH = asv_tab.baseline.NH[, colSums(asv_tab.baseline.NH) > 499]
asv_tab.baseline.NH = asv_tab.baseline.NH[rowSums(asv_tab.baseline.NH) > 0,] %>% t()
ncol(asv_tab.baseline.NH)
nrow(asv_tab.baseline.NH)
######################
#trap catch AND baseline samples IN NH
asv_tab.samps_and_baseline.NH = asv_tab[,colnames(asv_tab) %in% samps_and_baseline_ids.NH]
ncol(asv_tab.samps_and_baseline.NH) == length(samps_and_baseline_ids.NH)
samps_and_baseline_ids.NH[!samps_and_baseline_ids.NH  %in% colnames(asv_tab.samps_and_baseline.NH)]
#how many samples with lt 1k seqs
asv_tab.samps_and_baseline.NH[,colSums(asv_tab.samps_and_baseline.NH) > 999] %>% colnames %>% length
#114
colSums(asv_tab.samps_and_baseline.NH) %>% sort
asv_tab.samps_and_baseline.NH[,colSums(asv_tab.samps_and_baseline.NH) > 499] %>% colnames %>% length
#
#how many nonzero asvs
sum(rowSums(asv_tab.samps_and_baseline.NH) > 0)
#18230
sum(rowSums(asv_tab.samps_and_baseline.NH) > 1)
#16758
#
#remove 0 count asvs and samples with lt 1k seqs and transpose. Note we will also output the un filtered/unrarefied samples for purposes of *detection*
asv_tab.samps_and_baseline.NH = asv_tab.samps_and_baseline.NH[, colSums(asv_tab.samps_and_baseline.NH) > 499]
asv_tab.samps_and_baseline.NH = asv_tab.samps_and_baseline.NH[rowSums(asv_tab.samps_and_baseline.NH) > 0,] %>% t()
ncol(asv_tab.samps_and_baseline.NH)
nrow(asv_tab.samps_and_baseline.NH)
max(rowSums(asv_tab.samps))/min(rowSums(asv_tab.samps))
#904
max(rowSums(asv_tab.samps_and_baseline))/min(rowSums(asv_tab.samps_and_baseline))
#98
max(rowSums(asv_tab.samps_and_pcrNeg))/min(rowSums(asv_tab.samps_and_pcrNeg))
#1179
max(rowSums(asv_tab.samps_and_baseline.NH))/min(rowSums(asv_tab.samps_and_baseline.NH))
#2786
max(rowSums(asv_tab.baseline.NH))/min(rowSums(asv_tab.baseline.NH))
#sample type IDs
samples_ids.NH = metadata %>% filter(dateOrBaselineType != "BLO" & dateOrBaselineType != "BLF" & dateOrBaselineType != "negative" & State == "NH") %>% pull(SequenceID)
samples_ids.NH %>% length
samples_ids.NH
######################
#trap catch samples IN NH (for comparison to insects)
asv_tab.samps.NH = asv_tab[,colnames(asv_tab) %in% samples_ids.NH]
ncol(asv_tab.samps.NH) == length(samples_ids.NH)
samples_ids.NH[!samples_ids.NH  %in% colnames(asv_tab.samps.NH)]
#how many samples with lt 1k seqs
asv_tab.samps.NH[,colSums(asv_tab.samps.NH) > 999] %>% colnames %>% length
#100
colSums(asv_tab.samps.NH) %>% sort
asv_tab.samps.NH[,colSums(asv_tab.samps.NH) > 499] %>% colnames %>% length
#
#how many nonzero asvs
sum(rowSums(asv_tab.samps.NH) > 0)
#17966
sum(rowSums(asv_tab.samps.NH) > 1)
#16463
#
#remove 0 count asvs and samples with lt 1k seqs and transpose. Note we will also output the un filtered/unrarefied samples for purposes of *detection*
asv_tab.samps.NH = asv_tab.samps.NH[, colSums(asv_tab.samps.NH) > 499]
asv_tab.samps.NH = asv_tab.samps.NH[rowSums(asv_tab.samps.NH) > 0,] %>% t()
ncol(asv_tab.samps.NH)
nrow(asv_tab.samps.NH)
#236.9521
max(rowSums(asv_tab.samps.NH))/min(rowSums(asv_tab.samps.NH))
###############
#perform rarefactions
#
write.csv(asv_tab.samps, "data/FEDRR_all_2024/asv_tab.samps.csv", quote = F)
write.csv(asv_tab.samps_and_baseline, "data/FEDRR_all_2024/asv_tab.samps_and_baseline.csv", quote = F)
write.csv(asv_tab.samps_and_pcrNeg, "data/FEDRR_all_2024/asv_tab.samps_and_pcrNeg.csv", quote = F)
write.csv(asv_tab.samps_and_baseline.NH, "data/FEDRR_all_2024/asv_tab.samps_and_baseline.NH.csv", quote = F)
write.csv(asv_tab.baseline.NH, "data/FEDRR_all_2024/asv_tab.baseline.NH.csv", quote = F)
write.csv(asv_tab.samps.NH, "data/FEDRR_all_2024/asv_tab.samps.NH.csv", quote = F)
q(save="no")
?Reduce
?simplify2array
?matrix
matrix(data = 2, nrow = 2, ncol = 2)
twos = matrix(data = 2, nrow = 2, ncol = 2)
ones = matrix(data = 1, nrow = 2, ncol = 2)
twos
ones
twos + ones
mat_list = list()
mat_list[[1]] = ones
mat_list[[2]] = twos
Reduce(+, mat_list)
Reduce('+', mat_list)
system.time(foo = Reduce('+', mat_list))
system.time(foo <- Reduce('+', mat_list))
mat_list[[1:100]] = ones
foo <- Reduce('+', mat_list)
foo
foo/2
ones/2
twos/2
Reduce('as.matrix', mat_list)
as.matrix(mat_list)
lapply(mat_list, as.matrix)
q(save="no")
